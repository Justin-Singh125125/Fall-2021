{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6380c684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "['the']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.feature_extraction.text as sk_text\n",
    "\n",
    "# CountVectorizer implements both tokenization and occurrence counting in a single class:\n",
    "vectorizer = sk_text.CountVectorizer(min_df=1)\n",
    "\n",
    "corpus = ['This is the first document.',\n",
    "           'this is the second second document.',\n",
    "           'And the third one.',\n",
    "           'Is this the first first first document?',\n",
    "          ]\n",
    "\n",
    "matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# convert to numpy array\n",
    "matrix.toarray()\n",
    "\n",
    "# view all unique strings found in all string arrays\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "# TfidfVectorizer combines all the options of CountVectorizer and TfidfTransformer in a single model:\n",
    "# TfIdfVectorizer: transforms text into a \"sparse matrix\" where rows are text and columns are words, and values are the tf-dif values.\n",
    "vectorizer = sk_text.TfidfVectorizer(max_features=1000, min_df=1)\n",
    "matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "\n",
    "vectorizer = sk_text.TfidfVectorizer(min_df=4, max_df=1000)\n",
    "vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b08398a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.23684538 0.         0.         0.         0.29356375 0.\n",
      "  0.         0.         0.         0.         0.29356375 0.\n",
      "  0.         0.         0.23684538 0.         0.29356375 0.\n",
      "  0.         0.         0.29356375 0.29356375 0.         0.29356375\n",
      "  0.29356375 0.         0.         0.         0.         0.\n",
      "  0.         0.23684538 0.         0.         0.29356375 0.\n",
      "  0.29356375 0.         0.         0.23684538]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.29954105\n",
      "  0.         0.37127341 0.         0.37127341 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.29954105 0.         0.29954105 0.37127341 0.\n",
      "  0.         0.         0.         0.37127341 0.         0.\n",
      "  0.         0.         0.29954105 0.29954105]\n",
      " [0.36252618 0.44934185 0.         0.44934185 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.36252618 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.36252618 0.         0.         0.         0.\n",
      "  0.         0.44934185 0.         0.        ]\n",
      " [0.         0.         0.31156077 0.         0.         0.\n",
      "  0.31156077 0.31156077 0.         0.31156077 0.         0.25136527\n",
      "  0.31156077 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.31156077 0.\n",
      "  0.         0.         0.         0.25136527 0.         0.31156077\n",
      "  0.31156077 0.         0.         0.         0.         0.31156077\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.25643278\n",
      "  0.         0.         0.25643278 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.25643278\n",
      "  0.25643278 0.25643278 0.         0.         0.         0.\n",
      "  0.         0.4137767  0.51286555 0.         0.         0.\n",
      "  0.         0.         0.25643278 0.         0.         0.\n",
      "  0.         0.         0.4137767  0.        ]]\n",
      "(5, 40)\n",
      "['abe', 'abenomics', 'accelerating', 'asked', 'country', 'crazy', 'cuba', 'daily', 'deer', 'eased', 'economic', 'economy', 'falls', 'fix', 'future', 'hard', 'healing', 'horse', 'horses', 'hunting', 'japan', 'minister', 'obama', 'people', 'prime', 'putin', 'riding', 'ruble', 'russia', 'russian', 'sanctions', 'shinzo', 'things', 'tumbled', 'turmoil', 'value', 'view', 'views', 'vladimir', 'working']\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE 2\n",
    "\n",
    "document_0 = \"Japan's prime minister, Shinzo Abe, is working towards healing the economic turmoil in his own country for his view on the future of his people.\"\n",
    "document_1 = \"Vladimir Putin is working hard to fix the economy in Russia as the Ruble has tumbled.\"\n",
    "document_2 = \"What's the future of Abenomics? We asked Shinzo Abe for his views\"\n",
    "document_3 = \"Obama has eased sanctions on Cuba while accelerating those against the Russian Economy, even as the Ruble's value falls almost daily.\"\n",
    "document_4 = \"Vladimir Putin is riding a horse while hunting deer. Vladimir Putin always seems so serious about things - even riding horses. Is he crazy?\"\n",
    "\n",
    "\n",
    "corpus = [document_0, document_1, document_2, document_3, document_4]\n",
    "\n",
    "vectorizer = sk_text.TfidfVectorizer(stop_words='english',\n",
    "                             max_features = 100,\n",
    "                             min_df=1, \n",
    "                             )\n",
    "\n",
    "matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "tfidf_data = matrix.toarray()     #  convert it to numpy array\n",
    "\n",
    "print(tfidf_data)\n",
    "print(tfidf_data.shape)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2db405f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baca4d94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f01d30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
